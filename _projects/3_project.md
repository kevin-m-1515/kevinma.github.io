---
layout: page
title: external validity and machine learning
description: a deep dive into datasets 
img:
importance: 1
category: work
---

In machine learning, we generally work with the framework of train-test benchmarking. Our goal is to train a model such that we not only minimize the training error but also minimize the test error as well. However, there is this idea that we can "overfit" on the test set, which, in theory, should lead to a decrease in accuracy on an alternate test set with a similar distribution. What we discussed in lecture is that this idea is probably not true. However, this does not mean that machine learning generalizes well beyond the distribution of datasets it was trained on. I discuss this further in my notes, which can be seen [HERE!](https://kevinma1515.github.io/assets/pdf/Lecture_Scribe.pdf).

These notes were created based on a lecture by Ben Recht, who teaches the Statistical Learning Theory class at UC Berkeley. You can read his blog on this idea [here](https://argmin.substack.com/p/you-got-a-9-to-5-so-ill-take-the).
 



